{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX8zKrS-hRJk",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "# Práctica: Los Juegos del Hambre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XysGzsSAmBvu"
      },
      "source": [
        "ESTUDIANTE: `ADRIÁN FERNÁNDEZ UGUINA`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vanYztAMhRJt",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "<table><tr>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/breakfast.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/hamburger.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/fruits.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "</tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E7BW750hRJu",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "En esta práctica vamos a enfrentarnos a un problema desafiante de clasificación de imágenes, construyendo una red neuronal profunda que sea capaz de clasificar entre diferentes tipos de comida. ¡Que comiencen los Juegos del Hambre!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnn_FAm8hRJv",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "## Guidelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gspUM6n3hRJw"
      },
      "source": [
        "A lo largo del notebook encontrarás celdas que debes rellenar con tu propio código. Sigue las instrucciones del notebook y presta atención a los siguientes iconos:\n",
        "\n",
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Deberás resolver el ejercicio escribiendo tu propio código o respuesta en la celda inmediatamente inferior.</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZHQpQXrhRJw"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "Esto es una pista u observación de utilidad que puede ayudarte a resolver el ejercicio. Presta atención a estas pistas para comprender el ejercicio en mayor profundidad.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xghhJf_HhRJx"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "Este es un ejercicio avanzado que te puede ayudar a profundizar en el tema, y a conseguir una calificación más alta. ¡Buena suerte!</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWAbqrofhRJy"
      },
      "source": [
        "Para evitar problemas con imports o incompatibilidades se recomienda ejecutar este notebook en uno de los [entornos de Deep Learning recomendados](https://github.com/albarji/teaching-environments-deeplearning), o hacer uso [Google Colaboratory](https://colab.research.google.com/). Si usas Colaboratory, asegúrate de [conectar una GPU](https://colab.research.google.com/notebooks/gpu.ipynb), y de haber [deactivado otras sesiones que tuvieras activas](https://stackoverflow.com/a/53441194/2436578)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZmf8R9DhRJy"
      },
      "source": [
        "El siguiente código mostrará todas las gráficas en el propio notebook en lugar de generar una nueva ventana."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DxmdgWqNhRJz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2rxty2Ix2kF",
        "outputId": "bb96bd39-9e1a-423b-a9e9-eb02a6e0340a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_KRrPHmLx2kG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Evita mensajes de advertencia de TensorFlow\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Índice de la GPU a utilizar (0, 1, 2, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UgDu1WUhRJ0"
      },
      "source": [
        "Finalmente, si necesitas ayuda en el uso de cualquier función Python, coloca el cursor sobre su nombre y presiona Shift+Tab. Aparecerá una ventana con su documentación. Esto solo funciona dentro de celdas de código.\n",
        "\n",
        "¡Vamos alla!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K733uf0ghRJ2"
      },
      "source": [
        "## Obtención de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZYPquwFhRJ2"
      },
      "source": [
        "Vamos a usar un dataset de imágenes de comida disponible en [Kaggle](https://www.kaggle.com/trolukovich/food11-image-dataset). Para descargarlo, necesitarás crear una cuenta de usuario en Kaggle, y obtener tus credenciales de la API. Puedes hacerlo siguiendo las instrucciones de [esta sección](https://github.com/Kaggle/kaggle-api#api-credentials). ¡Ojo! Tus credenciales de la API no son lo mismo que la contraseña que utilizas para acceder a tu cuenta en Kaggle.\n",
        "\n",
        "Una vez tengas el fichero JSON con tus credenciales, puedes declararlas en este notebook asignando las variables de entorno adecuadas, de la siguiente manera\n",
        "\n",
        "    import os\n",
        "\n",
        "    os.environ[\"KAGGLE_USERNAME\"] = \"YOUR KAGGLE USERNAME HERE\"\n",
        "    os.environ[\"KAGGLE_KEY\"] = \"YOUR KAGGLE KEY HERE\"\n",
        "    \n",
        "Cuando lo hayas hecho, podrás descargar el dataset a la máquina donde esté corriendo este notebook usando el siguiente comando\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "donde debes indicar el nombre de un directorio válido como \"YOUR_LOCAL_FOLDER\". Si prefieres descargar los datos en la misma carpeta que este notebook, puedes quitar la parte `-p YOUR_LOCAL_FOLDER` del comando."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRX8BQsZhRJ2"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Crea tu propia cuenta de Kaggle (si no tienes ya una), obtén tus credenciales, y usa la celda inferior para declarar tu nombre de usuario de Kaggle y tu clave como variables de entorno. A continuación, usa la misma celda para descargar el dataset de imágenes.\n",
        "    \n",
        "¡Ojo! Debes mantener estas credenciales en secreto, ya que son personales a tu usuario de Kaggle. Recuerda borrarlas de la celda antes de entregar este notebook.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiFYSULfhRJ3",
        "outputId": "d0ff60a9-1376-400a-a308-fe16e6f34049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading food11-image-dataset.zip to /content\n",
            " 98% 1.07G/1.08G [00:09<00:00, 211MB/s]\n",
            "100% 1.08G/1.08G [00:09<00:00, 118MB/s]\n"
          ]
        }
      ],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"adrinfernndezuguina\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"ade9590da40cdfa403674d914d8dd141\"\n",
        "\n",
        "!kaggle datasets download trolukovich/food11-image-dataset --unzip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmh27gIEhRJ4"
      },
      "source": [
        "Revisa ahora la carpeta en la que has descargado los datos. Verás que contiene 3 subdirectorios:\n",
        "\n",
        "* **training**, contiene las imágenes a utilizar para entrenar el modelo.\n",
        "* **validation**, contiene imágenes adicionales que podrías usar como datos de entrenamiento adicionales, o para algún tipo de estrategia de validación como Early Stopping.\n",
        "* **evaluation**, contiene las imágenes que debes utilizar para testear el modelo. Las imágenes de esta carpeta **solo** pueden utilizarse para medir el rendimiento del modelo tras su entrenamiento, y para nada más.\n",
        "\n",
        "Además de esto, dentro de cada una de estas carpetas encontrarás una subcarpeta para cada una de las 11 clases de comida:\n",
        "\n",
        "* Bread (panes)\n",
        "* Dairy product (lácteos)\n",
        "* Dessert (postres)\n",
        "* Egg (huevos)\n",
        "* Fried food (fritos)\n",
        "* Meat (carnes)\n",
        "* Noodles-Pasta (pasta)\n",
        "* Rice (arroz)\n",
        "* Seafood (pescado y marisco)\n",
        "* Soup (sopas)\n",
        "* Vegetable-Fruit (vegetales y frutas)\n",
        "\n",
        "Esta es una forma estándar de organizar los datasets de imágenes: una carpeta para cada clase. Para facilitar los pasos de procesamiento que vendrán a continuación, vamos a definir algunas variables que nos indiquen dónde están almacenados los diferentes conjuntos de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJrE8F8zhRJ4"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Crea variables <b>TRAINDIR</b>, <b>VALDIR</b> y <b>TESTDIR</b>, cada una conteniendo una cadena de texto con la ruta al directorio donde están los datos de entrenamiento, validación y evaluación, respectivamente.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TzK2VI3ShRJ5"
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "TRAINDIR = \"/content//training\"\n",
        "VALDIR = \"/content//validation\"\n",
        "TESTDIR = \"/content//evaluation\"\n",
        "\n",
        "if not os.path.isdir(TRAINDIR):\n",
        "    print(f\"El directorio de entrenamiento '{TRAINDIR}' no existe.\")\n",
        "if not os.path.isdir(VALDIR):\n",
        "    print(f\"El directorio de validación '{VALDIR}' no existe.\")                                                                                                     \n",
        "if not os.path.isdir(TESTDIR):\n",
        "    print(f\"El directorio de test '{TESTDIR}' no existe.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikYtQyR-hRJ6"
      },
      "source": [
        "### Reducción de clases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWsLot-7hRJ6"
      },
      "source": [
        "Con el fin de hacer este problema más accesible de cara a la práctica, vamos a centrarnos solo en seis de las clases de comida disponibles: `Bread`, `Dairy product`, `Dessert`, `Egg`, `Fried food` y `Meat`. Para ello, se provee el código siguiente, que elimina de los datos descargados las carpetas correspondientes a imágenes de las otras clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXoNw1-thRJ7",
        "outputId": "c3fd2a3a-2590-4d77-f630-0e8fa7a4af43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting /content//evaluation/Vegetable-Fruit...\n",
            "Deleting /content//evaluation/Noodles-Pasta...\n",
            "Deleting /content//evaluation/Soup...\n",
            "Deleting /content//evaluation/Seafood...\n",
            "Deleting /content//evaluation/Rice...\n",
            "Deleting /content//training/Vegetable-Fruit...\n",
            "Deleting /content//training/Noodles-Pasta...\n",
            "Deleting /content//training/Soup...\n",
            "Deleting /content//training/Seafood...\n",
            "Deleting /content//training/Rice...\n",
            "Deleting /content//validation/Vegetable-Fruit...\n",
            "Deleting /content//validation/Noodles-Pasta...\n",
            "Deleting /content//validation/Soup...\n",
            "Deleting /content//validation/Seafood...\n",
            "Deleting /content//validation/Rice...\n"
          ]
        }
      ],
      "source": [
        "from glob import glob\n",
        "import os\n",
        "\n",
        "valid_classes = {\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\", \"Meat\"}\n",
        "datasets = {TRAINDIR, VALDIR, TESTDIR}\n",
        "\n",
        "for dataset in datasets:\n",
        "    for classdir in glob(f\"{dataset}/*\"):  # Find subfolders with classes\n",
        "        if classdir.split(\"/\")[-1] not in valid_classes:  # Ignore those in valid_classes\n",
        "            print(f\"Deleting {classdir}...\")\n",
        "            for fname in glob(f\"{classdir}/*.jpg\"):  # Remove each image file\n",
        "                os.remove(fname)\n",
        "            os.rmdir(classdir)  # Remove folder\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fncCfOYihRJ7"
      },
      "source": [
        "## Procesando imágenes desde ficheros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wTWrPr-hRJ7"
      },
      "source": [
        "Este dataset de imágenes es grande, con imágenes de mayor resolución que las que hemos utilizado en el tutorial del MNIST, y cada una de ellas teniendo diferentes tamaños y relación de aspecto. Además, mientras que para el MNIST teníamos una función de keras que preparaba los datos para nosotros, en esta ocasión tendremos que realizar el trabajo de carga y procesamiento de las imágenes.\n",
        "\n",
        "Una forma conveniente de hacer todo este trabajo es a través de la función Keras `image_dataset_from_directory`. Esta función crea un objeto `Dataset` de TensorFlow con todas las imágenes de un directorio, cargándolas en memoria de forma dinámica solo cuando la red neuronal necesita utilizarlas. Esta función también nos permite especificar algunas opciones de preprocesamiento muy útiles.\n",
        "\n",
        "Por ejemplo, podemos crear un `Dataset` con los datos en la carpeta de training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-edpP7-NhRJ8",
        "outputId": "26653b9b-dea7-4463-9288-30e19e69eda3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6082 files belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "\n",
        "image_size = 128\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    TRAINDIR, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = 'categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGEdEuF5hRJ8"
      },
      "source": [
        "Observa los parámetros que se han utilizado para configurar el dataset:\n",
        "\n",
        "* El **directorio** desde el que cargar las imágenes.\n",
        "* Un **tamaño de imagen (image_size)** que se utilizará para redimensionar todas las imágenes cargadas a ese tamaño común, en este caso 32x32 píxeles.\n",
        "* El **tamaños de los lotes (batch_size)** de imágenes a ser generados. Nótese que definimos aquí este parametro en lugar de en el paso `fit` de la red, como hemos hecho en otros ejercicios, porque el objeto `Dataset` resultante hará uso de esta información para mantener en memoria solo algunos batches de imágenes, ahorrando así memoria.\n",
        "* El **modo de etiquetado (label_mode)**, esto es, la codificación de las etiquetas a utilizar. `categorical` significa que utilizaremos la ya conocida codificación one-hot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgeGjQDVhRJ9"
      },
      "source": [
        "Un objeto `Dataset` funciona de manera muy similar a un generador de Python, lo que significa que podemos iterar sobre él para obtener batches de imágenes ya preprocesadas. Por ejemplo, el siguiente código inicia un bucle para extraer todos los batches del `Dataset`, nos muestra el contenido el primero, y detiene la iteración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPAIrHu7hRJ9",
        "outputId": "2f312f3c-2de6-4f53-9aac-d8f3e47e3bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input batch: (64, 128, 128, 3)\n",
            "Shape of output batch: (64, 6)\n",
            "Input batch:\n",
            "[[[[2.60000000e+01 1.15000000e+01 8.50000000e+00]\n",
            "   [2.57500000e+01 1.12500000e+01 6.00000000e+00]\n",
            "   [7.17500000e+01 5.65000000e+01 4.47500000e+01]\n",
            "   ...\n",
            "   [1.02250000e+02 6.32500000e+01 3.12500000e+01]\n",
            "   [1.03250000e+02 6.50000000e+01 2.97500000e+01]\n",
            "   [1.06250000e+02 6.82500000e+01 3.15000000e+01]]\n",
            "\n",
            "  [[2.52500000e+01 1.32500000e+01 1.20000000e+01]\n",
            "   [2.52500000e+01 1.20000000e+01 1.07500000e+01]\n",
            "   [2.67500000e+01 1.35000000e+01 5.00000000e+00]\n",
            "   ...\n",
            "   [1.00750000e+02 6.20000000e+01 2.82500000e+01]\n",
            "   [1.04250000e+02 6.62500000e+01 2.97500000e+01]\n",
            "   [1.05250000e+02 6.72500000e+01 2.92500000e+01]]\n",
            "\n",
            "  [[2.62500000e+01 1.25000000e+01 1.10000000e+01]\n",
            "   [2.77500000e+01 1.27500000e+01 1.25000000e+01]\n",
            "   [2.92500000e+01 1.42500000e+01 8.00000000e+00]\n",
            "   ...\n",
            "   [9.77500000e+01 6.12500000e+01 3.07500000e+01]\n",
            "   [1.04750000e+02 6.87500000e+01 3.37500000e+01]\n",
            "   [1.04000000e+02 6.85000000e+01 3.15000000e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.87750000e+02 1.80250000e+02 1.61000000e+02]\n",
            "   [2.17000000e+02 2.10250000e+02 1.89750000e+02]\n",
            "   [2.50750000e+02 2.45000000e+02 2.20000000e+02]\n",
            "   ...\n",
            "   [1.87000000e+02 1.96500000e+02 1.88750000e+02]\n",
            "   [1.85250000e+02 1.94750000e+02 1.87000000e+02]\n",
            "   [2.11500000e+02 2.21000000e+02 2.13250000e+02]]\n",
            "\n",
            "  [[2.10000000e+01 1.10000000e+01 2.50000000e+00]\n",
            "   [1.08500000e+02 9.85000000e+01 8.75000000e+01]\n",
            "   [1.86250000e+02 1.78250000e+02 1.61750000e+02]\n",
            "   ...\n",
            "   [2.05750000e+02 2.19250000e+02 2.07750000e+02]\n",
            "   [2.09750000e+02 2.23250000e+02 2.11750000e+02]\n",
            "   [1.90000000e+02 2.03500000e+02 1.92000000e+02]]\n",
            "\n",
            "  [[1.80000000e+01 7.00000000e+00 4.25000000e+00]\n",
            "   [2.02500000e+01 6.75000000e+00 2.00000000e+00]\n",
            "   [3.17500000e+01 1.47500000e+01 7.75000000e+00]\n",
            "   ...\n",
            "   [1.97250000e+02 2.11750000e+02 1.93750000e+02]\n",
            "   [2.01000000e+02 2.15500000e+02 1.97500000e+02]\n",
            "   [2.07250000e+02 2.21750000e+02 2.03750000e+02]]]\n",
            "\n",
            "\n",
            " [[[2.19750000e+02 2.49750000e+02 2.18250000e+02]\n",
            "   [2.16750000e+02 2.53000000e+02 2.17500000e+02]\n",
            "   [2.17500000e+02 2.51250000e+02 2.15500000e+02]\n",
            "   ...\n",
            "   [2.25000000e+01 1.60000000e+01 3.90000000e+01]\n",
            "   [2.17500000e+01 1.87500000e+01 4.57500000e+01]\n",
            "   [3.32500000e+01 2.57500000e+01 5.37500000e+01]]\n",
            "\n",
            "  [[2.21000000e+02 2.50250000e+02 2.16000000e+02]\n",
            "   [2.22500000e+02 2.53000000e+02 2.18000000e+02]\n",
            "   [2.25250000e+02 2.51750000e+02 2.16250000e+02]\n",
            "   ...\n",
            "   [1.95000000e+01 1.82500000e+01 4.40000000e+01]\n",
            "   [2.47500000e+01 2.12500000e+01 4.77500000e+01]\n",
            "   [3.15000000e+01 2.10000000e+01 4.77500000e+01]]\n",
            "\n",
            "  [[2.26000000e+02 2.52500000e+02 2.16500000e+02]\n",
            "   [2.28000000e+02 2.51500000e+02 2.16000000e+02]\n",
            "   [2.34500000e+02 2.51750000e+02 2.16500000e+02]\n",
            "   ...\n",
            "   [2.47500000e+01 2.27500000e+01 4.70000000e+01]\n",
            "   [2.50000000e+01 1.80000000e+01 4.30000000e+01]\n",
            "   [3.07500000e+01 1.85000000e+01 4.62500000e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.58750000e+02 3.97500000e+01 1.32500000e+01]\n",
            "   [1.53250000e+02 3.22500000e+01 1.22500000e+01]\n",
            "   [1.50250000e+02 2.65000000e+01 1.15000000e+01]\n",
            "   ...\n",
            "   [1.36000000e+02 5.42500000e+01 4.45000000e+01]\n",
            "   [1.20000000e+02 3.85000000e+01 3.50000000e+01]\n",
            "   [1.10750000e+02 2.97500000e+01 3.22500000e+01]]\n",
            "\n",
            "  [[1.63250000e+02 4.32500000e+01 2.07500000e+01]\n",
            "   [1.49750000e+02 2.87500000e+01 1.12500000e+01]\n",
            "   [1.52000000e+02 2.82500000e+01 1.35000000e+01]\n",
            "   ...\n",
            "   [1.13250000e+02 3.32500000e+01 2.97500000e+01]\n",
            "   [1.35500000e+02 5.72500000e+01 4.87500000e+01]\n",
            "   [1.31250000e+02 5.42500000e+01 4.20000000e+01]]\n",
            "\n",
            "  [[1.42250000e+02 1.77500000e+01 3.00000000e+00]\n",
            "   [1.44500000e+02 2.05000000e+01 7.00000000e+00]\n",
            "   [1.50250000e+02 2.87500000e+01 1.27500000e+01]\n",
            "   ...\n",
            "   [1.27750000e+02 5.00000000e+01 4.22500000e+01]\n",
            "   [1.19750000e+02 4.20000000e+01 3.80000000e+01]\n",
            "   [1.15000000e+02 3.87500000e+01 3.70000000e+01]]]\n",
            "\n",
            "\n",
            " [[[7.91210938e+01 6.96093750e+01 5.19140625e+01]\n",
            "   [9.18281250e+01 7.54375000e+01 5.79375000e+01]\n",
            "   [8.87304688e+01 7.87187500e+01 6.30234375e+01]\n",
            "   ...\n",
            "   [1.75000000e+02 1.53902344e+02 1.41316406e+02]\n",
            "   [1.73500000e+02 1.54695312e+02 1.44097656e+02]\n",
            "   [1.68304688e+02 1.55195312e+02 1.46097656e+02]]\n",
            "\n",
            "  [[1.02605469e+02 8.35703125e+01 7.07773438e+01]\n",
            "   [1.06398438e+02 9.27773438e+01 7.64843750e+01]\n",
            "   [1.13398438e+02 9.52773438e+01 7.98632812e+01]\n",
            "   ...\n",
            "   [1.74207031e+02 1.54414062e+02 1.46828125e+02]\n",
            "   [1.74792969e+02 1.54207031e+02 1.44207031e+02]\n",
            "   [1.72207031e+02 1.53207031e+02 1.46207031e+02]]\n",
            "\n",
            "  [[1.15628906e+02 9.66406250e+01 8.05820312e+01]\n",
            "   [1.24558594e+02 1.06523438e+02 8.65117188e+01]\n",
            "   [1.28546875e+02 1.09070312e+02 9.05703125e+01]\n",
            "   ...\n",
            "   [1.75976562e+02 1.57976562e+02 1.46953125e+02]\n",
            "   [1.74464844e+02 1.58023438e+02 1.46511719e+02]\n",
            "   [1.73511719e+02 1.56511719e+02 1.48511719e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.69511719e+02 1.40511719e+02 1.24535156e+02]\n",
            "   [1.67523438e+02 1.38035156e+02 1.25476562e+02]\n",
            "   [1.65546875e+02 1.37546875e+02 1.19070312e+02]\n",
            "   ...\n",
            "   [1.90812500e+02 1.76253906e+02 1.64777344e+02]\n",
            "   [1.84210938e+02 1.57222656e+02 1.38175781e+02]\n",
            "   [1.94312500e+02 1.63312500e+02 1.39765625e+02]]\n",
            "\n",
            "  [[1.64207031e+02 1.36414062e+02 1.21792969e+02]\n",
            "   [1.65914062e+02 1.38207031e+02 1.23328125e+02]\n",
            "   [1.67707031e+02 1.37707031e+02 1.19292969e+02]\n",
            "   ...\n",
            "   [2.00308594e+02 1.83308594e+02 1.82343750e+02]\n",
            "   [1.84878906e+02 1.49792969e+02 1.33707031e+02]\n",
            "   [1.82242188e+02 1.43156250e+02 1.21691406e+02]]\n",
            "\n",
            "  [[1.65195312e+02 1.36195312e+02 1.21195312e+02]\n",
            "   [1.64902344e+02 1.36000000e+02 1.17707031e+02]\n",
            "   [1.68695312e+02 1.38195312e+02 1.23085938e+02]\n",
            "   ...\n",
            "   [1.94292969e+02 1.81292969e+02 1.76183594e+02]\n",
            "   [1.93378906e+02 1.61367188e+02 1.46757812e+02]\n",
            "   [1.83792969e+02 1.44378906e+02 1.17988281e+02]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[4.64882812e+01 2.09960938e+01 1.49609375e+00]\n",
            "   [1.98496094e+02 1.28492188e+02 5.47265625e+00]\n",
            "   [2.06023438e+02 1.48019531e+02 2.49609375e+00]\n",
            "   ...\n",
            "   [8.95664062e+01 5.50664062e+01 6.55664062e+01]\n",
            "   [9.00078125e+01 5.20156250e+01 6.35234375e+01]\n",
            "   [7.10507812e+01 4.60703125e+01 6.15703125e+01]]\n",
            "\n",
            "  [[4.80351562e+01 2.54882812e+01 3.51562500e-02]\n",
            "   [1.95628906e+02 1.36976562e+02 1.80000000e+01]\n",
            "   [1.95628906e+02 1.41582031e+02 5.02343750e+00]\n",
            "   ...\n",
            "   [1.45296875e+02 1.07355469e+02 1.18843750e+02]\n",
            "   [1.48089844e+02 1.06101562e+02 1.17148438e+02]\n",
            "   [1.47320312e+02 1.14402344e+02 1.27925781e+02]]\n",
            "\n",
            "  [[4.18125000e+01 2.62343750e+01 5.85937500e-02]\n",
            "   [1.64605469e+02 1.24390625e+02 2.14218750e+01]\n",
            "   [1.66792969e+02 1.26117188e+02 1.01679688e+01]\n",
            "   ...\n",
            "   [1.53039062e+02 1.15000000e+02 1.24597656e+02]\n",
            "   [1.27792969e+02 8.47929688e+01 9.37929688e+01]\n",
            "   [1.43558594e+02 1.03636719e+02 1.14636719e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[6.23320312e+01 1.46953125e+01 4.69531250e+00]\n",
            "   [4.65000000e+01 9.44140625e+00 2.92187500e+00]\n",
            "   [3.64804688e+01 1.49804688e+01 8.48046875e+00]\n",
            "   ...\n",
            "   [8.07617188e+01 6.56445312e+01 4.48203125e+01]\n",
            "   [3.57460938e+01 1.96679688e+01 5.78515625e+00]\n",
            "   [2.51757812e+01 1.06562500e+01 2.73437500e+00]]\n",
            "\n",
            "  [[7.90585938e+01 2.15468750e+01 1.05234375e+01]\n",
            "   [4.81171875e+01 8.11718750e+00 9.37500000e-02]\n",
            "   [3.40117188e+01 1.05117188e+01 1.51171875e+00]\n",
            "   ...\n",
            "   [3.85351562e+01 1.69882812e+01 1.54687500e+00]\n",
            "   [3.50000000e+01 1.35000000e+01 6.00000000e+00]\n",
            "   [2.99531250e+01 1.24648438e+01 6.92968750e+00]]\n",
            "\n",
            "  [[7.99765625e+01 2.34765625e+01 7.47656250e+00]\n",
            "   [5.24882812e+01 1.34882812e+01 1.98828125e+00]\n",
            "   [3.65039062e+01 1.35117188e+01 2.00781250e+00]\n",
            "   ...\n",
            "   [3.69921875e+01 1.29882812e+01 3.00000000e+00]\n",
            "   [3.44726562e+01 1.04726562e+01 4.97265625e+00]\n",
            "   [2.84882812e+01 1.04804688e+01 4.96093750e-01]]]\n",
            "\n",
            "\n",
            " [[[8.85000000e+01 8.85000000e+01 8.05000000e+01]\n",
            "   [8.75000000e+01 8.75000000e+01 7.95000000e+01]\n",
            "   [8.77500000e+01 8.77500000e+01 7.97500000e+01]\n",
            "   ...\n",
            "   [3.37500000e+01 3.70000000e+01 3.07500000e+01]\n",
            "   [2.90000000e+01 3.10000000e+01 2.35000000e+01]\n",
            "   [6.15000000e+01 6.30000000e+01 5.47500000e+01]]\n",
            "\n",
            "  [[9.55000000e+01 9.55000000e+01 8.75000000e+01]\n",
            "   [9.25000000e+01 9.25000000e+01 8.45000000e+01]\n",
            "   [9.15000000e+01 9.15000000e+01 8.35000000e+01]\n",
            "   ...\n",
            "   [3.77500000e+01 3.67500000e+01 3.17500000e+01]\n",
            "   [2.65000000e+01 2.45000000e+01 1.95000000e+01]\n",
            "   [3.55000000e+01 3.15000000e+01 2.60000000e+01]]\n",
            "\n",
            "  [[9.60000000e+01 9.60000000e+01 8.80000000e+01]\n",
            "   [9.22500000e+01 9.22500000e+01 8.42500000e+01]\n",
            "   [8.97500000e+01 8.97500000e+01 8.17500000e+01]\n",
            "   ...\n",
            "   [5.15000000e+01 4.90000000e+01 4.40000000e+01]\n",
            "   [3.45000000e+01 3.07500000e+01 2.65000000e+01]\n",
            "   [5.65000000e+01 5.15000000e+01 4.80000000e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.09500000e+02 2.06750000e+02 1.90750000e+02]\n",
            "   [2.01250000e+02 1.94750000e+02 1.77750000e+02]\n",
            "   [1.84500000e+02 1.73750000e+02 1.54750000e+02]\n",
            "   ...\n",
            "   [1.83750000e+02 1.60750000e+02 1.49250000e+02]\n",
            "   [1.87000000e+02 1.64500000e+02 1.52750000e+02]\n",
            "   [1.93750000e+02 1.71250000e+02 1.59500000e+02]]\n",
            "\n",
            "  [[2.03000000e+02 2.02750000e+02 1.90000000e+02]\n",
            "   [2.12000000e+02 2.08500000e+02 1.92250000e+02]\n",
            "   [2.08750000e+02 2.01750000e+02 1.79250000e+02]\n",
            "   ...\n",
            "   [1.94250000e+02 1.72250000e+02 1.59250000e+02]\n",
            "   [2.02500000e+02 1.82750000e+02 1.69000000e+02]\n",
            "   [2.04000000e+02 1.85500000e+02 1.71500000e+02]]\n",
            "\n",
            "  [[2.04500000e+02 2.05000000e+02 1.99250000e+02]\n",
            "   [2.08500000e+02 2.05500000e+02 1.93500000e+02]\n",
            "   [2.19000000e+02 2.13500000e+02 1.92500000e+02]\n",
            "   ...\n",
            "   [2.04250000e+02 1.82250000e+02 1.68250000e+02]\n",
            "   [2.12750000e+02 1.94750000e+02 1.79250000e+02]\n",
            "   [2.14250000e+02 1.98000000e+02 1.82000000e+02]]]\n",
            "\n",
            "\n",
            " [[[1.45500000e+02 1.10500000e+02 8.50000000e+01]\n",
            "   [1.46500000e+02 1.11000000e+02 8.00000000e+01]\n",
            "   [1.48500000e+02 1.13500000e+02 7.80000000e+01]\n",
            "   ...\n",
            "   [5.50000000e+00 3.50000000e+00 4.50000000e+00]\n",
            "   [7.50000000e+00 5.50000000e+00 6.50000000e+00]\n",
            "   [6.50000000e+00 4.50000000e+00 5.50000000e+00]]\n",
            "\n",
            "  [[1.48000000e+02 1.13000000e+02 8.85000000e+01]\n",
            "   [1.52000000e+02 1.16000000e+02 8.75000000e+01]\n",
            "   [1.51000000e+02 1.16500000e+02 8.30000000e+01]\n",
            "   ...\n",
            "   [5.50000000e+00 3.50000000e+00 4.50000000e+00]\n",
            "   [5.00000000e+00 3.00000000e+00 4.00000000e+00]\n",
            "   [5.00000000e+00 3.00000000e+00 4.00000000e+00]]\n",
            "\n",
            "  [[1.53500000e+02 1.18500000e+02 9.40000000e+01]\n",
            "   [1.52500000e+02 1.16500000e+02 8.80000000e+01]\n",
            "   [1.49000000e+02 1.15500000e+02 8.50000000e+01]\n",
            "   ...\n",
            "   [4.00000000e+00 2.00000000e+00 3.00000000e+00]\n",
            "   [5.50000000e+00 3.50000000e+00 4.50000000e+00]\n",
            "   [6.50000000e+00 4.50000000e+00 5.50000000e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.11500000e+02 1.68500000e+02 1.25500000e+02]\n",
            "   [2.20000000e+02 1.77500000e+02 1.31000000e+02]\n",
            "   [2.17000000e+02 1.74500000e+02 1.28000000e+02]\n",
            "   ...\n",
            "   [1.97000000e+02 1.65500000e+02 1.18500000e+02]\n",
            "   [1.95500000e+02 1.66000000e+02 1.19500000e+02]\n",
            "   [1.94500000e+02 1.62500000e+02 1.17000000e+02]]\n",
            "\n",
            "  [[2.13500000e+02 1.69500000e+02 1.30500000e+02]\n",
            "   [2.20000000e+02 1.77000000e+02 1.34000000e+02]\n",
            "   [2.15500000e+02 1.72500000e+02 1.27500000e+02]\n",
            "   ...\n",
            "   [1.97500000e+02 1.66000000e+02 1.19500000e+02]\n",
            "   [1.94000000e+02 1.64500000e+02 1.18000000e+02]\n",
            "   [1.95500000e+02 1.63500000e+02 1.20000000e+02]]\n",
            "\n",
            "  [[2.08500000e+02 1.64500000e+02 1.29500000e+02]\n",
            "   [2.14000000e+02 1.70000000e+02 1.31000000e+02]\n",
            "   [2.13500000e+02 1.70500000e+02 1.28500000e+02]\n",
            "   ...\n",
            "   [1.97500000e+02 1.65500000e+02 1.20000000e+02]\n",
            "   [1.95000000e+02 1.65500000e+02 1.20000000e+02]\n",
            "   [1.94500000e+02 1.62500000e+02 1.19000000e+02]]]]\n",
            "Output batch:\n",
            "[[0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]]\n"
          ]
        }
      ],
      "source": [
        "for X_batch, y_batch in train_dataset:\n",
        "    print(f\"Shape of input batch: {X_batch.shape}\")\n",
        "    print(f\"Shape of output batch: {y_batch.shape}\")\n",
        "    print(f\"Input batch:\\n{X_batch}\")\n",
        "    print(f\"Output batch:\\n{y_batch}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2UaPCEthRJ-"
      },
      "source": [
        "Podemos ver que, efectivamente, el generador produce un tensor de datos de entrada de las dimensiones apropiadas para poder introducirlo en la red neuronal, y que las salidas también se han codificado correctamente como one-hot.\n",
        "No obstante, todavía hay un problema con los datos: los valores de los píxeles están en el rango [0, 255], lo cual puede producir problemas de entrenamiento. Resolveremos este punto después, en la definición de la red neuronal, mediante una capa especial. Por ahora vamos a cotinuar, definiendo una función que construya los `Dataset` para los datos de entrenamiento, validación y test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTQyBbf-hRJ-"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Crea una función <b>create_datasets</b> que reciba los siguiente parámetros:\n",
        "    <ul>\n",
        "      <li><b>traindir</b>: el directorio donde están localizadas las imágenes de entrenamiento.</li>\n",
        "      <li><b>valdir</b>: el directorio donde están localizadas las imágenes de validación.</li>\n",
        "      <li><b>testdir</b>: el directorio donde están localizadas las imágenes de test.</li>\n",
        "      <li><b>image_size</b>: el tamaño que se utilizará para redimensionar todas las imágenes a una resolución común.</li>\n",
        "      <li><b>batch_size</b>: el tamaño de los batches de imágenes que serán generados.</li>\n",
        "    </ul>\n",
        "    La función debe crear objetos `Dataset` para los directorios de entrenamiento, validación y test, y devolver los tres datasets creados como\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "D-wm19eJhRJ-"
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "\n",
        "\n",
        "def create_datasets(traindir, valdir, testdir, image_size, batch_size):\n",
        "    if isinstance(image_size, int):\n",
        "        image_size = (image_size, image_size)\n",
        "\n",
        "    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        traindir,\n",
        "        image_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical'\n",
        "    )\n",
        "\n",
        "    val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        valdir,\n",
        "        image_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical'\n",
        "    )\n",
        "\n",
        "    test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        testdir,\n",
        "        image_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical'\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wGaULQsjwJz"
      },
      "source": [
        "Probemos que la función que has implementado funciona correctamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtZlNbHhiOIR",
        "outputId": "9a561224-b90e-4277-bf89-a57312ffc20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=128, batch_size=64) \n",
        "\n",
        "# Test whether all returned objects are valid Tensorflow datasets\n",
        "assert isinstance(train_dataset, tf.data.Dataset)\n",
        "assert isinstance(val_dataset, tf.data.Dataset)\n",
        "assert isinstance(test_dataset, tf.data.Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gbwmYJiLhRJ_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, Convolution2D, Rescaling\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model.add(Convolution2D(4, 3, activation='linear'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLIo6LBfhRJ-"
      },
      "source": [
        "¡Ahora que tenemos nuestros `Dataset` podemos entrenar una red profunda con ellos! Como ejemplo, vamos a construir una red convolucional extremadamente simple. Nótese cómo hemos añadido una capa especial de preprocesado llamada `Rescaling`, que será la encargada de normalizar los valores de los píxeles al rango [0, 1] cada vez que la red reciba una imagen.\n",
        "\n",
        "¡Ojo! Esta red tan simple no producirá errores al ejecutar, pero tiene algunos fallos de diseño que deberás corregir cuando crees tu propia red, más adelante en esta práctica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v9OxA-AhRJ_"
      },
      "source": [
        "El método `fit` de un modelo Keras puede recibir un objeto `Dataset` como datos de entrenamiento, en lugar de un par de tensores (entradas, salidas). Como al construir los `Dataset` ya especificamos el tamaño de batch, no es necesario indicarlo ahora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZXjs6CghRKA",
        "outputId": "9d23ae39-6048-4bda-bb13-c5ff31f348b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 6s 55ms/step - loss: nan - accuracy: 0.1664       \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1b19885e830>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_ux64pchRKA"
      },
      "source": [
        "Análogamente, podemos evaluar el rendimiento de nuestro modelo sobre el `Dataset` de test de la siguiente manera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_kQhkY-hRKA",
        "outputId": "61cf694f-6050-4661-98f6-b620deaa8d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 2s 38ms/step - loss: nan - accuracy: 0.1778\n",
            "Loss nan, accuracy 17.8%\n"
          ]
        }
      ],
      "source": [
        "loss, acc = model.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlXf9OBehRKB"
      },
      "source": [
        "Este nivel de acierto puede parecer pobre, pero ten en cuenta que hemos usado un modelo muy simple y que el problema es de 6 clases. ¿Serás capaz de hacerlo mejor?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kZnaNQIhRKB"
      },
      "source": [
        "## Construyendo tu propia red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNd8dC2whRKB"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Diseña una red neuronal profunda que obtenga el mejor acierto posible sobre los datos de test. Puedes usar los datos de entrenamiento y validación como te parezca mejor, pero <b>sólo</b> puedes usar los datos de test para evaluar el acierto de tu modelo. Debes obtener una red capaz de alcanzar al menos un 45% de acierto sobre los datos de test.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9C9GNYihRKC"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "    \n",
        "Algunas recomendaciones y estrategias que pueden ayudar a mejorar tu diseño de red:\n",
        "    \n",
        "- <b>Arquitectura</b>: usa todos los trucos que has aprendido en los ejercicios anteriores: capas convolucionales + pooling, activaciones ReLU, dropout... asegúrate también de utilizar un buen optimizador, con una función de error (loss) adecuada, así como una función de activación en la capa de salida que sea adecuada para esta clase de problema (clasificación multiclase).\n",
        "- <b>Desarrollo incremental</b>: empieza por redes pequeñas, con un número pequeño de parámetros, de forma que puedas comprobar rápidamente qué tal funcionan. Después, puedes hacer tu red más grande en tres direcciones: mayor tamaño de imágenes de entrada, más capas, y más kernels por capa convolucional o unidades por capa densa. Si aumentas el tamaño de las imágenes de entrada, asegúrate de añadir también más capas Convolution+Pooling, para que así a la capa Flatten solo lleguen imágenes muy pequeñas (10x10 píxeles o menos).\n",
        "- <b>Tamaño de imágenes</b>: configurar los `Dataset` para que carguen imágenes de mayor tamaño puede mejorar significativamente el rendimiento de tu red. Pero ten cuidado, ¡también puedes encontrarte errores de falta de memoria (CUDA memory error) si cargas imágenes a un tamaño demasiado grande! Para esta práctica, un tamaño mayor a 256 puede ser demasiado grande...\n",
        "- <b>Controlar el número de épocas</b>: Usa una <a href=\"https://keras.io/api/callbacks/early_stopping/\">**estrategia de EarlyStopping**</a> para monitorizar el loss de los datos de validación, y así detener el entrenamiento cuando tras un número de épocas esa loss no haya decrecido. Configurar la EarlyStopping para restaurar los mejores parámetros encontrados durante la optimización también puede resultarte útil.\n",
        "- <b>Sobreajuste</b>: si tu red obtiene un accuracy casi perfecto en entrenamiento, puede que estés sufriendo sobreajuste (aunque puede que no...). Prueba a incrementar el nivel de Dropout en las capas Dense para comprobar si así obtienes mejoras en el test.\n",
        "- <b>Demasiado bueno para ser verdad</b>: si tu red obtiene resultados muy buenos, del orden del 90% o más de acierto en test... sospecha. Es posible que estés mezclando los datos de entrenamiento y test.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssDs0SGghRKC"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "    \n",
        "Como ejercicio avanzado, añade las siguiente estrategias a tu red:\n",
        "\n",
        "- Usa **técnicas de \"image augmentation\"** para aumentar artificialmente tu dataset de entrenamiento. Para ello, explora las <a href=\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/\">capas de augmentation disponibles en Keras</a>.\n",
        "- Usa capas de <a href=\"https://keras.io/api/layers/normalization_layers/batch_normalization/\">BatchNormalization</a> para facilitar el entrenamiento de la red. Revisa en las diapositivas de clase cuál es la forma adecuada de colocarlas en la red.\n",
        "    \n",
        "Usando estos trucos y los mencionados en el punto anterior, es posible obtener más de un 60% de acierto en el conjunto de test.\n",
        "\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0werKiOwx2kS"
      },
      "source": [
        "MODELO 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Jgz7w3VqhRKC"
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "\n",
        "from tensorflow.keras.layers import  MaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import  MaxPooling2D\n",
        "from tensorflow.keras.layers import  BatchNormalization\n",
        "from tensorflow.keras.layers import RandomFlip, RandomCrop, RandomTranslation, RandomRotation, RandomZoom\n",
        "from tensorflow.keras.layers import  BatchNormalization\n",
        "\n",
        "\n",
        "model1 = Sequential()\n",
        "\n",
        "\n",
        "model1.add(Rescaling(scale=1./255, input_shape=(128, 128, 3)))\n",
        "\n",
        "model1.add(Convolution2D(64, (3, 3), input_shape=(image_size, image_size, 3)))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "model1.add(Convolution2D(32, (5, 5),  input_shape=(image_size, image_size, 3)))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Activation('relu'))\n",
        "\n",
        "model1.add(Convolution2D(32, (5, 5),  input_shape=(image_size, image_size, 3)))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Activation('relu'))\n",
        "\n",
        "model1.add(Convolution2D(32, (5, 5),  input_shape=(image_size, image_size, 3)))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "model1.add(Flatten())\n",
        "\n",
        "model1.add(Dense(2042, activation=\"relu\"))\n",
        "model1.add(Dropout(0.6))\n",
        "\n",
        "\n",
        "model1.add(Dense(6, activation=\"softmax\"))\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True)\n",
        "\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OExWxW9x2kS",
        "outputId": "89d5e039-100e-4c4d-85d2-d836cae0558a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "96/96 [==============================] - 19s 152ms/step - loss: 11.0783 - accuracy: 0.2695 - val_loss: 1.8359 - val_accuracy: 0.1822\n",
            "Epoch 2/30\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 1.5893 - accuracy: 0.3287 - val_loss: 1.9105 - val_accuracy: 0.1736\n",
            "Epoch 3/30\n",
            "96/96 [==============================] - 16s 157ms/step - loss: 1.5324 - accuracy: 0.3454 - val_loss: 1.8865 - val_accuracy: 0.2049\n",
            "Epoch 4/30\n",
            "96/96 [==============================] - 14s 140ms/step - loss: 1.5136 - accuracy: 0.3578 - val_loss: 1.8776 - val_accuracy: 0.1689\n",
            "Epoch 5/30\n",
            "96/96 [==============================] - 15s 146ms/step - loss: 1.4800 - accuracy: 0.3842 - val_loss: 1.6790 - val_accuracy: 0.2766\n",
            "Epoch 6/30\n",
            "96/96 [==============================] - 14s 140ms/step - loss: 1.4619 - accuracy: 0.4012 - val_loss: 1.6335 - val_accuracy: 0.3031\n",
            "Epoch 7/30\n",
            "96/96 [==============================] - 15s 145ms/step - loss: 1.5055 - accuracy: 0.3757 - val_loss: 1.4414 - val_accuracy: 0.4042\n",
            "Epoch 8/30\n",
            "96/96 [==============================] - 14s 142ms/step - loss: 1.4602 - accuracy: 0.3944 - val_loss: 1.4809 - val_accuracy: 0.4080\n",
            "Epoch 9/30\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 1.4634 - accuracy: 0.3792 - val_loss: 1.4036 - val_accuracy: 0.4440\n",
            "Epoch 10/30\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 1.4273 - accuracy: 0.3979 - val_loss: 1.4097 - val_accuracy: 0.4165\n",
            "Epoch 11/30\n",
            "96/96 [==============================] - 14s 141ms/step - loss: 1.4138 - accuracy: 0.4211 - val_loss: 1.5482 - val_accuracy: 0.4084\n",
            "Epoch 12/30\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 1.4041 - accuracy: 0.4314 - val_loss: 1.4506 - val_accuracy: 0.4179\n",
            "Epoch 13/30\n",
            "96/96 [==============================] - 15s 150ms/step - loss: 1.3914 - accuracy: 0.4354 - val_loss: 1.3862 - val_accuracy: 0.4611\n",
            "Epoch 14/30\n",
            "96/96 [==============================] - 14s 142ms/step - loss: 1.3724 - accuracy: 0.4421 - val_loss: 1.4134 - val_accuracy: 0.4535\n",
            "Epoch 15/30\n",
            "96/96 [==============================] - 14s 142ms/step - loss: 1.3360 - accuracy: 0.4505 - val_loss: 1.7789 - val_accuracy: 0.3795\n",
            "Epoch 16/30\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 1.3532 - accuracy: 0.4571 - val_loss: 1.4654 - val_accuracy: 0.4696\n",
            "Epoch 17/30\n",
            "96/96 [==============================] - 15s 153ms/step - loss: 1.3425 - accuracy: 0.4573 - val_loss: 1.3456 - val_accuracy: 0.4986\n",
            "Epoch 18/30\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 1.3354 - accuracy: 0.4648 - val_loss: 1.5257 - val_accuracy: 0.4545\n",
            "Epoch 19/30\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 1.3203 - accuracy: 0.4688 - val_loss: 1.3473 - val_accuracy: 0.4682\n",
            "Epoch 20/30\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 1.3005 - accuracy: 0.4780 - val_loss: 1.3797 - val_accuracy: 0.4497\n",
            "Epoch 21/30\n",
            "96/96 [==============================] - 15s 149ms/step - loss: 1.3039 - accuracy: 0.4762 - val_loss: 1.3427 - val_accuracy: 0.4749\n",
            "Epoch 22/30\n",
            "96/96 [==============================] - 14s 142ms/step - loss: 1.3807 - accuracy: 0.4380 - val_loss: 1.3903 - val_accuracy: 0.4620\n",
            "Epoch 23/30\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 1.3373 - accuracy: 0.4622 - val_loss: 1.3439 - val_accuracy: 0.4910\n",
            "Epoch 24/30\n",
            "96/96 [==============================] - 17s 167ms/step - loss: 1.2762 - accuracy: 0.4845 - val_loss: 1.3960 - val_accuracy: 0.4625\n",
            "Epoch 25/30\n",
            "96/96 [==============================] - 14s 142ms/step - loss: 1.2849 - accuracy: 0.4898 - val_loss: 1.9260 - val_accuracy: 0.3956\n",
            "Epoch 26/30\n",
            "96/96 [==============================] - 17s 168ms/step - loss: 1.5142 - accuracy: 0.3680 - val_loss: 1.5279 - val_accuracy: 0.3956\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc9d04c61d0>"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "model1.fit(train_dataset, \n",
        "           batch_size=128, \n",
        "           epochs=30, \n",
        "           validation_data=val_dataset, \n",
        "           callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdFxST7mx2kT",
        "outputId": "8cf2a274-c92f-45fa-9fa4-7db0da39d09b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 6s 150ms/step - loss: 1.3238 - accuracy: 0.4792\n",
            "Loss 1.32, accuracy 47.9%\n"
          ]
        }
      ],
      "source": [
        "loss, acc = model1.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVr9dmJ9x2kT"
      },
      "source": [
        "<H2> Accuracy en el conjunto de test usando un modelo simple: 47.9% <h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl1kx62-x2kT"
      },
      "source": [
        "MODELO 2  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_mN48Zp-x2kT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.layers import  MaxPooling2D\n",
        "from tensorflow.keras.layers import  BatchNormalization\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import RandomFlip, RandomCrop, RandomTranslation\n",
        "from tensorflow.keras.layers import RandomRotation, RandomZoom\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        "\n",
        "#model2.add(RandomFlip(mode=\"horizontal\"))\n",
        "#model2.add(RandomZoom(height_factor=(-0.3, 0.3), width_factor=(-0.3, 0.3)))\n",
        "#model2.add(RandomRotation(factor=(-0.3, 0.3)))\n",
        "#model2.add(RandomTranslation(height_factor=(-0.3, 0.3), width_factor=(-0.3, 0.3)))\n",
        "#model2.add(RandomCrop(height=32, width=32))\n",
        "\n",
        "model2.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "\n",
        "model2.add(Convolution2D(256, (3, 3), input_shape=(image_size, image_size, 3)))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model2.add(Convolution2D(128,(3, 3)))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model2.add(Convolution2D(64, (3, 3)))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model2.add(Flatten())\n",
        "\n",
        "model2.add(Dense(256, activation='relu'))\n",
        "model2.add(Dropout(0.5))\n",
        "\n",
        "model2.add(Dense(128, activation='relu'))\n",
        "model2.add(Dropout(0.5))\n",
        "\n",
        "model2.add(Dense(6, activation='softmax'))\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True)\n",
        "\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "Bf7OC42kx2kT",
        "outputId": "72f47ca5-a315-4392-f500-e03dcf25a310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "96/96 [==============================] - 34s 269ms/step - loss: 2.1608 - accuracy: 0.2552 - val_loss: 1.7651 - val_accuracy: 0.2391\n",
            "Epoch 2/50\n",
            "96/96 [==============================] - 25s 250ms/step - loss: 1.7122 - accuracy: 0.2711 - val_loss: 1.7483 - val_accuracy: 0.2372\n",
            "Epoch 3/50\n",
            "96/96 [==============================] - 25s 250ms/step - loss: 1.7136 - accuracy: 0.2604 - val_loss: 1.7418 - val_accuracy: 0.2372\n",
            "Epoch 4/50\n",
            "96/96 [==============================] - 25s 250ms/step - loss: 1.7197 - accuracy: 0.2601 - val_loss: 1.7386 - val_accuracy: 0.2372\n",
            "Epoch 5/50\n",
            "96/96 [==============================] - 25s 249ms/step - loss: 1.6954 - accuracy: 0.2657 - val_loss: 1.7382 - val_accuracy: 0.2362\n",
            "Epoch 6/50\n",
            "96/96 [==============================] - 25s 251ms/step - loss: 1.6958 - accuracy: 0.2650 - val_loss: 1.7178 - val_accuracy: 0.2467\n",
            "Epoch 7/50\n",
            "96/96 [==============================] - 25s 252ms/step - loss: 1.6744 - accuracy: 0.2766 - val_loss: 1.6627 - val_accuracy: 0.2742\n",
            "Epoch 8/50\n",
            "96/96 [==============================] - 25s 252ms/step - loss: 1.6892 - accuracy: 0.2657 - val_loss: 1.6491 - val_accuracy: 0.2647\n",
            "Epoch 9/50\n",
            "96/96 [==============================] - 25s 253ms/step - loss: 1.6752 - accuracy: 0.2754 - val_loss: 1.6118 - val_accuracy: 0.2936\n",
            "Epoch 10/50\n",
            "96/96 [==============================] - 25s 252ms/step - loss: 1.6701 - accuracy: 0.2774 - val_loss: 1.6625 - val_accuracy: 0.2751\n",
            "Epoch 11/50\n",
            "96/96 [==============================] - 25s 252ms/step - loss: 1.6629 - accuracy: 0.2772 - val_loss: 1.6285 - val_accuracy: 0.2742\n",
            "Epoch 12/50\n",
            "96/96 [==============================] - 25s 253ms/step - loss: 1.6590 - accuracy: 0.2775 - val_loss: 1.7133 - val_accuracy: 0.2690\n",
            "Epoch 13/50\n",
            "96/96 [==============================] - 26s 259ms/step - loss: 1.6682 - accuracy: 0.2751 - val_loss: 1.6673 - val_accuracy: 0.3031\n",
            "Epoch 14/50\n",
            "96/96 [==============================] - 25s 259ms/step - loss: 1.6726 - accuracy: 0.2767 - val_loss: 1.6126 - val_accuracy: 0.2946\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-3011685ea43a>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m            callbacks=[early_stopping])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mreproducir_sonido\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-94-4b65d81332f4>\u001b[0m in \u001b[0;36mreproducir_sonido\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreproducir_sonido\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msound_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rate must be specified when data is a numpy array or list of audio samples.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: rate must be specified when data is a numpy array or list of audio samples."
          ]
        }
      ],
      "source": [
        "model2.fit(train_dataset, \n",
        "           batch_size=batch_size, \n",
        "           epochs=50, \n",
        "           validation_data=val_dataset, \n",
        "           callbacks=[early_stopping])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64mIQHDyx2kU",
        "outputId": "73f3b1c6-bbec-44eb-a2dc-bd2c1cb38ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 5s 106ms/step - loss: 1.6075 - accuracy: 0.2981\n",
            "Loss 1.61, accuracy 29.8%\n"
          ]
        }
      ],
      "source": [
        "loss, acc = model2.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\") \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrLe9mFwhRKC"
      },
      "source": [
        "## Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a5xs6_RhRKC"
      },
      "source": [
        "Aunque diseñar nuestra propia red puede producir algunos resultados aceptables, suele ser mejor aprovechar el conocimiento ya existente en una red pre-entrenada. Esto no solo nos lleva a resultados mejores, sino que además nos ahorra mucho tiempo de diseño de la red. Para ello, el módulo [Keras Applications](https://keras.io/api/applications/) contiene varios diseños de redes listos para su uso. Por ejemplo, para hacer uso de la famosa red VGG16 hacemos lo siguiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTLIqMTAhRKD",
        "outputId": "80e53ce0-5411-441f-b354-304f6a3aeded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "vgg16_model = VGG16(include_top=False, input_shape=(image_size, image_size, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZUvx4zdhRKD"
      },
      "source": [
        "Por defecto, todas las redes de Keras Applications están precargadas con los pesos que se obtuvieron al entrenar la red sobre el dataset de la competición de [ImageNet](http://www.image-net.org/). Para adaptar la red a nuestro problema, hemos necesitado especificar la resolución de nuestras imágenes (`input_shape`), así como eliminar las capas de salida (`top`) de la red original, dado que nosotros tendremos un número diferente de clases.\n",
        "\n",
        "Ahora, ¿cómo hacemos para transferir el aprendizaje de esta red? Vamos a ver cómo implementar la estrategia de \"bottleneck features\". En primer lugar, marcamos el modelo VGG16 como no entrenable, para que sus parámetros se mantengan congelados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iWsuQg0phRKD"
      },
      "outputs": [],
      "source": [
        "vgg16_model.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljtS1PhRhRKE"
      },
      "source": [
        "Hecho esto, vamos a construir una red neuronal que incluya la VGG16 como una de sus \"capas\". Es necesario tener en cuenta que la red VGG16 se entrenó realizando una normalización muy específica de las imágenes de entrenamiento, y nosotros debemos seguir ese mismo proceso para que la red se comporte correctamente. Convenientemente, Keras también nos da una funcionalidad para replicar la normalización que la VGG16 necesita."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZjFpoNRIhRKE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTBk5uRChRKF"
      },
      "source": [
        "Podemos probar esta normalización con alguna de las imágenes de nuestro dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6ukW26shRKF",
        "outputId": "b40c7889-aa33-4888-ad13-7c18d6aefe88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before normalizing: [[[241. 255. 251.]\n",
            "  [241. 255. 251.]\n",
            "  [241. 255. 251.]]\n",
            "\n",
            " [[241. 255. 251.]\n",
            "  [241. 255. 251.]\n",
            "  [241. 255. 251.]]\n",
            "\n",
            " [[241. 255. 251.]\n",
            "  [241. 255. 251.]\n",
            "  [241. 255. 251.]]]\n",
            "After normalizing: [[[147.061   138.22101 117.32   ]\n",
            "  [147.061   138.22101 117.32   ]\n",
            "  [147.061   138.22101 117.32   ]]\n",
            "\n",
            " [[147.061   138.22101 117.32   ]\n",
            "  [147.061   138.22101 117.32   ]\n",
            "  [147.061   138.22101 117.32   ]]\n",
            "\n",
            " [[147.061   138.22101 117.32   ]\n",
            "  [147.061   138.22101 117.32   ]\n",
            "  [147.061   138.22101 117.32   ]]]\n"
          ]
        }
      ],
      "source": [
        "for X_batch, _ in train_dataset:\n",
        "    break\n",
        "    \n",
        "print(f\"Before normalizing: {X_batch[0, :3, :3, :]}\")\n",
        "print(f\"After normalizing: {preprocess_input(X_batch)[0, :3, :3, :]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx3EQeEThRKF"
      },
      "source": [
        "La normalización realizada por la VGG16 consiste en invertir el orden de los canales de color (RGB -> BGR), y restar los valores medios sobre todo el dataset ImageNet para cada canal de color por separado. Afortunadamente, la función `preprocess_input` que hemos importado hace todo este trabajo por nosotros. Además, podemos incrustar esta función como la primera capa de nuestra red, cumpliendo el papel de la capa `Rescaling` que utilizamos en el apartado anterior. Esto es posible gracias a la capa `Lambda`, que permite construir una capa Keras en base a cualquier función de Tensorflow. De modo que, vamos a comenzar nuestro diseño de red con esta capa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DnSoUUrQhRKG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k83-_MmdhRKG"
      },
      "source": [
        "Tras esto, podemos añadir toda la red VGG16 como si fuera una nueva capa, y nuestras propias capas después de ella. A continuación tenemos un ejemplo de esta clase de diseño, aunque es importante destacar que es un diseño muy sencillo que contiene algunos fallos; una red real para hacer transfer learning debería tener un diseño mejor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f0RdEumhRKG",
        "outputId": "440c1c9c-c33e-4ed0-e14e-9164429f2d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda (Lambda)             (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,717,766\n",
            "Trainable params: 3,078\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.add(vgg16_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NueAGF7DhRKG"
      },
      "source": [
        "Observa cómo en el resumen del modelo podemos ver que la red completa tiene millones de parámetros, pero dado que hemos congelado toda la parte de la red perteneciente a la VGG16, solo unos pocos miles de parámetros son entrenables (trainable): aquellos correspondientes a la capa Dense que hemos colocado al final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn7iMKlQhRKH"
      },
      "source": [
        "Ya podemos compilar y entrenar el modelo a la manera habitual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eBG6zKwhRKH",
        "outputId": "d6025928-b2a5-4645-8fc7-3408589b0054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 21s 213ms/step - loss: 67.8851 - accuracy: 0.3403\n",
            "33/33 [==============================] - 9s 246ms/step - loss: 226.0015 - accuracy: 0.1865\n",
            "Loss 2.26e+02, accuracy 18.6%\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])\n",
        "model.fit(train_dataset, epochs=1)\n",
        "\n",
        "loss, acc = model.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sb7SVSkhRKH"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Usando la estrategia \"bottleneck\" presentada, implementa una red que haga transfer learning desde la red VGG16, con un diseño correcto. Si lo haces adecuadamente, esta red debe obtener mejores resultados que con la red que diseñaste en el apartado anterior, y con al menos un 80% de acierto sobre el conjunto de test.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm2LQa-mhRKI"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "    \n",
        "Algunos consejos para mejorar tu diseño de red:\n",
        "    \n",
        "- Incluye una o más capas Dense, con sus funciones de activación apropiadas, antes de la capa de salida.\n",
        "- Intenta usar una capa de tipo [GlobalAveragePooling](https://keras.io/api/layers/pooling_layers/global_average_pooling2d/) en lugar de la capa Flatten. Esta capa calcula una media de todos los valores de píxeles para cada canal, y en algunas ocasiones produce mejores resultados que la capa Flatten.\n",
        "- ¡Y no olvides todos los consejos del apartado anterior! También aplican aquí.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6bbHEWhRKI"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "\n",
        "Para mejorar aún más los resultados de tu red, utiliza las siguientes ideas:\n",
        "\n",
        "- Usa las estrategias PRO del ejercicio anterio.\n",
        "- Prueba otras redes pre-entrenadas de <a href=\"https://keras.io/api/applications/\">Keras Applications</a>, como ResNet, Xception o EfficientNet.\n",
        "- Usa una estrategia de transfer learning más avanzada, como fine-tuning o una combinación de bottleneck features y fine-tuning. Revisa las diapositivas de clase para saber cómo.\n",
        "   \n",
        "Si empleas todos estos trucos, es posible alcanzar más de un 90% de acierto en el conjunto de test.\n",
        "\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nGiuQ8wqx2kX"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "s5yvgc2Mx2kX"
      },
      "outputs": [],
      "source": [
        "model_pro = Sequential()\n",
        "model_pro.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qLJ8IgANhRKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3d87e9-7d86-4438-9df4-583df53aa06d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda_2 (Lambda)           (None, 128, 128, 3)       0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 512)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 4096)              2101248   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2048)              8390656   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 6)                 12294     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,218,886\n",
            "Trainable params: 10,504,198\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "\n",
        "model_pro.add(vgg16_model)\n",
        "\n",
        "\n",
        "#model_pro.add(Flatten())\n",
        "model_pro.add(GlobalAveragePooling2D())\n",
        "\n",
        "model_pro.add(Dense(4096, activation=\"relu\"))\n",
        "model_pro.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model_pro.add(Dense(2048, activation=\"relu\"))\n",
        "model_pro.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "model_pro.add(Dense(6, activation=\"softmax\"))\n",
        "\n",
        "early_stopping = EarlyStopping(patience=7, monitor='val_loss', restore_best_weights=True)\n",
        "model_pro.summary()\n",
        "\n",
        "\n",
        "\n",
        "optimazer = Adam(learning_rate=0.0001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-yq3FeDx2kY",
        "outputId": "1cd0ac6d-e495-49ad-9813-0fda8ecee046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "96/96 [==============================] - 17s 148ms/step - loss: 3.0728 - accuracy: 0.5197 - val_loss: 0.9705 - val_accuracy: 0.7201\n",
            "Epoch 2/100\n",
            "96/96 [==============================] - 15s 150ms/step - loss: 1.4905 - accuracy: 0.6432 - val_loss: 0.7696 - val_accuracy: 0.7453\n",
            "Epoch 3/100\n",
            "96/96 [==============================] - 14s 142ms/step - loss: 1.0990 - accuracy: 0.6835 - val_loss: 0.7547 - val_accuracy: 0.7381\n",
            "Epoch 4/100\n",
            "96/96 [==============================] - 15s 147ms/step - loss: 0.9160 - accuracy: 0.7156 - val_loss: 0.7071 - val_accuracy: 0.7547\n",
            "Epoch 5/100\n",
            "96/96 [==============================] - 14s 144ms/step - loss: 0.7761 - accuracy: 0.7371 - val_loss: 0.7323 - val_accuracy: 0.7396\n",
            "Epoch 6/100\n",
            "96/96 [==============================] - 14s 139ms/step - loss: 0.6811 - accuracy: 0.7632 - val_loss: 0.6863 - val_accuracy: 0.7680\n",
            "Epoch 7/100\n",
            "96/96 [==============================] - 14s 144ms/step - loss: 0.6356 - accuracy: 0.7798 - val_loss: 0.6731 - val_accuracy: 0.7699\n",
            "Epoch 8/100\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 0.5499 - accuracy: 0.8006 - val_loss: 0.6701 - val_accuracy: 0.7638\n",
            "Epoch 9/100\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 0.5093 - accuracy: 0.8239 - val_loss: 0.6525 - val_accuracy: 0.7742\n",
            "Epoch 10/100\n",
            "96/96 [==============================] - 16s 157ms/step - loss: 0.4553 - accuracy: 0.8326 - val_loss: 0.6425 - val_accuracy: 0.7785\n",
            "Epoch 11/100\n",
            "96/96 [==============================] - 14s 142ms/step - loss: 0.4170 - accuracy: 0.8443 - val_loss: 0.6420 - val_accuracy: 0.7832\n",
            "Epoch 12/100\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 0.3529 - accuracy: 0.8722 - val_loss: 0.6316 - val_accuracy: 0.7880\n",
            "Epoch 13/100\n",
            "96/96 [==============================] - 14s 142ms/step - loss: 0.3231 - accuracy: 0.8782 - val_loss: 0.6214 - val_accuracy: 0.7870\n",
            "Epoch 14/100\n",
            "96/96 [==============================] - 14s 144ms/step - loss: 0.2881 - accuracy: 0.8944 - val_loss: 0.6337 - val_accuracy: 0.7804\n",
            "Epoch 15/100\n",
            "96/96 [==============================] - 14s 141ms/step - loss: 0.2769 - accuracy: 0.8959 - val_loss: 0.6545 - val_accuracy: 0.7794\n",
            "Epoch 16/100\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 0.2319 - accuracy: 0.9143 - val_loss: 0.6500 - val_accuracy: 0.7808\n",
            "Epoch 17/100\n",
            "96/96 [==============================] - 14s 142ms/step - loss: 0.2074 - accuracy: 0.9249 - val_loss: 0.6519 - val_accuracy: 0.7865\n",
            "Epoch 18/100\n",
            "96/96 [==============================] - 16s 158ms/step - loss: 0.1848 - accuracy: 0.9352 - val_loss: 0.6580 - val_accuracy: 0.7837\n",
            "Epoch 19/100\n",
            "96/96 [==============================] - 14s 142ms/step - loss: 0.1590 - accuracy: 0.9438 - val_loss: 0.6761 - val_accuracy: 0.7766\n",
            "Epoch 20/100\n",
            "96/96 [==============================] - 14s 143ms/step - loss: 0.1717 - accuracy: 0.9380 - val_loss: 0.6942 - val_accuracy: 0.7804\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc99071fd60>"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ],
      "source": [
        "\n",
        "model_pro.compile(loss='categorical_crossentropy', optimizer=optimazer, metrics=[\"accuracy\"])\n",
        "model_pro.fit(train_dataset, \n",
        "           batch_size=64, \n",
        "           epochs=100, \n",
        "           validation_data=val_dataset, \n",
        "           callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SE4u1u8x2kY",
        "outputId": "921eb2cf-d8bb-4321-cb03-db140d5de411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 4s 95ms/step - loss: 0.6098 - accuracy: 0.7947\n",
            "Loss 0.61, accuracy 79.5%\n"
          ]
        }
      ],
      "source": [
        "loss, acc = model_pro.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "\n",
        "Efficient_model = EfficientNetB7(include_top=False, input_shape=(image_size, image_size, 3))"
      ],
      "metadata": {
        "id": "jhfczp0jNkZD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Efficient_model.trainable = False"
      ],
      "metadata": {
        "id": "p6TsndfNOZw9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UIXa6CijOVkl"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X_batch, _ in train_dataset:\n",
        "    break\n",
        "    \n",
        "print(f\"Before normalizing: {X_batch[0, :3, :3, :]}\")\n",
        "print(f\"After normalizing: {preprocess_input(X_batch)[0, :3, :3, :]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CuabmWzPa6-",
        "outputId": "404305a5-979b-4f58-d9fb-af2fcc9bdcca"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before normalizing: [[[38.5  25.   15.  ]\n",
            "  [41.5  24.   10.5 ]\n",
            "  [58.75 34.25 16.75]]\n",
            "\n",
            " [[38.5  25.25 16.5 ]\n",
            "  [42.5  25.5  12.  ]\n",
            "  [61.75 39.75 22.25]]\n",
            "\n",
            " [[41.75 30.25 21.5 ]\n",
            "  [40.75 25.75 13.75]\n",
            "  [52.   31.   15.5 ]]]\n",
            "After normalizing: [[[38.5  25.   15.  ]\n",
            "  [41.5  24.   10.5 ]\n",
            "  [58.75 34.25 16.75]]\n",
            "\n",
            " [[38.5  25.25 16.5 ]\n",
            "  [42.5  25.5  12.  ]\n",
            "  [61.75 39.75 22.25]]\n",
            "\n",
            " [[41.75 30.25 21.5 ]\n",
            "  [40.75 25.75 13.75]\n",
            "  [52.   31.   15.5 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pro2 = Sequential()\n",
        "model_pro2.add(Lambda(preprocess_input, input_shape=(128, 128, 3)))"
      ],
      "metadata": {
        "id": "A9AIdSnXITwi"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_pro.add(Flatten())\n",
        "\n",
        "model_pro2.add(Efficient_model)\n",
        "\n",
        "model_pro2.add(GlobalAveragePooling2D())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_pro2.add(Dense(2048, activation=\"relu\"))\n",
        "model_pro2.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model_pro2.add(Dense(1024, activation=\"relu\"))\n",
        "model_pro2.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model_pro2.add(Dense(512, activation=\"relu\"))\n",
        "model_pro2.add(Dropout(0.5))\n",
        "\n",
        "#model_pro2.add(Dense(32, activation=\"relu\"))\n",
        "#model_pro2.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_pro2.add(Dense(6, activation=\"softmax\"))\n",
        "\n",
        "early_stopping = EarlyStopping(patience=7, monitor='val_loss', restore_best_weights=True)\n",
        "model_pro2.summary()\n",
        "\n",
        "\n",
        "\n",
        "optimazer = Adam(learning_rate=0.0001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AucubLTpIcIp",
        "outputId": "1c2d5f10-2fd4-4a0c-a837-377cad0d2aba"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda_3 (Lambda)           (None, 128, 128, 3)       0         \n",
            "                                                                 \n",
            " efficientnetb7 (Functional)  (None, 4, 4, 2560)       64097687  \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 2560)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 2048)              5244928   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1024)              2098176   \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 71,968,669\n",
            "Trainable params: 7,870,982\n",
            "Non-trainable params: 64,097,687\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pro2.compile(loss='categorical_crossentropy', optimizer=optimazer, metrics=[\"accuracy\"])\n",
        "model_pro2.fit(train_dataset, \n",
        "           batch_size=64, \n",
        "           epochs=150, \n",
        "           validation_data=val_dataset, \n",
        "           callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwg8Ydn9Ij7r",
        "outputId": "16fde91e-9817-4b67-871a-438287414703"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "96/96 [==============================] - 60s 382ms/step - loss: 1.3985 - accuracy: 0.4622 - val_loss: 0.9218 - val_accuracy: 0.6494\n",
            "Epoch 2/150\n",
            "96/96 [==============================] - 32s 323ms/step - loss: 0.9701 - accuracy: 0.6463 - val_loss: 0.7698 - val_accuracy: 0.7173\n",
            "Epoch 3/150\n",
            "96/96 [==============================] - 32s 331ms/step - loss: 0.8482 - accuracy: 0.6896 - val_loss: 0.7129 - val_accuracy: 0.7391\n",
            "Epoch 4/150\n",
            "96/96 [==============================] - 32s 328ms/step - loss: 0.7494 - accuracy: 0.7330 - val_loss: 0.6986 - val_accuracy: 0.7410\n",
            "Epoch 5/150\n",
            "96/96 [==============================] - 32s 328ms/step - loss: 0.7026 - accuracy: 0.7489 - val_loss: 0.6728 - val_accuracy: 0.7562\n",
            "Epoch 6/150\n",
            "96/96 [==============================] - 32s 327ms/step - loss: 0.6452 - accuracy: 0.7641 - val_loss: 0.6546 - val_accuracy: 0.7581\n",
            "Epoch 7/150\n",
            "96/96 [==============================] - 32s 331ms/step - loss: 0.6165 - accuracy: 0.7780 - val_loss: 0.6310 - val_accuracy: 0.7723\n",
            "Epoch 8/150\n",
            "96/96 [==============================] - 32s 324ms/step - loss: 0.5609 - accuracy: 0.7969 - val_loss: 0.6339 - val_accuracy: 0.7799\n",
            "Epoch 9/150\n",
            "96/96 [==============================] - 32s 328ms/step - loss: 0.5373 - accuracy: 0.8119 - val_loss: 0.6283 - val_accuracy: 0.7818\n",
            "Epoch 10/150\n",
            "96/96 [==============================] - 32s 327ms/step - loss: 0.4904 - accuracy: 0.8259 - val_loss: 0.6230 - val_accuracy: 0.7865\n",
            "Epoch 11/150\n",
            "96/96 [==============================] - 32s 324ms/step - loss: 0.4497 - accuracy: 0.8479 - val_loss: 0.6580 - val_accuracy: 0.7751\n",
            "Epoch 12/150\n",
            "96/96 [==============================] - 32s 327ms/step - loss: 0.4414 - accuracy: 0.8435 - val_loss: 0.6190 - val_accuracy: 0.7998\n",
            "Epoch 13/150\n",
            "96/96 [==============================] - 32s 328ms/step - loss: 0.4065 - accuracy: 0.8527 - val_loss: 0.6239 - val_accuracy: 0.7884\n",
            "Epoch 14/150\n",
            "96/96 [==============================] - 34s 344ms/step - loss: 0.3936 - accuracy: 0.8596 - val_loss: 0.6259 - val_accuracy: 0.7927\n",
            "Epoch 15/150\n",
            "96/96 [==============================] - 32s 326ms/step - loss: 0.3422 - accuracy: 0.8815 - val_loss: 0.6137 - val_accuracy: 0.7993\n",
            "Epoch 16/150\n",
            "96/96 [==============================] - 34s 344ms/step - loss: 0.3239 - accuracy: 0.8867 - val_loss: 0.6198 - val_accuracy: 0.7984\n",
            "Epoch 17/150\n",
            "96/96 [==============================] - 32s 325ms/step - loss: 0.3017 - accuracy: 0.8925 - val_loss: 0.6836 - val_accuracy: 0.7951\n",
            "Epoch 18/150\n",
            "96/96 [==============================] - 32s 324ms/step - loss: 0.2976 - accuracy: 0.8941 - val_loss: 0.6633 - val_accuracy: 0.8046\n",
            "Epoch 19/150\n",
            "96/96 [==============================] - 32s 324ms/step - loss: 0.2751 - accuracy: 0.9055 - val_loss: 0.6273 - val_accuracy: 0.8126\n",
            "Epoch 20/150\n",
            "96/96 [==============================] - 32s 325ms/step - loss: 0.2424 - accuracy: 0.9161 - val_loss: 0.6587 - val_accuracy: 0.8069\n",
            "Epoch 21/150\n",
            "96/96 [==============================] - 34s 346ms/step - loss: 0.2283 - accuracy: 0.9178 - val_loss: 0.6620 - val_accuracy: 0.8164\n",
            "Epoch 22/150\n",
            "96/96 [==============================] - 32s 330ms/step - loss: 0.2023 - accuracy: 0.9303 - val_loss: 0.7266 - val_accuracy: 0.8060\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6877dd9de0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model_pro2.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erB-8_beIotZ",
        "outputId": "799580b4-75e1-4784-dc1f-be9a86ba9590"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 9s 231ms/step - loss: 0.5702 - accuracy: 0.8217\n",
            "Loss 0.57, accuracy 82.2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJc7DMJhRKK"
      },
      "source": [
        "## Informe final y resumen de resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV2c2JCKhRKK"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Escribe en la siguiente celda un pequeño informe con:\n",
        "    <ul>\n",
        "        <li>Una tabla de resultados, indicando qué diseños de red has probado y qué resultados en test has obtenido. Puede usar un estilo de tabla como el que se muestra abajo.</li>\n",
        "        <li>De las estrategias y diseños que has ido probando, ¿qué ha funcionado y qué no?</li>\n",
        "        <li>¿Qué has aprendido con esta práctica?\n",
        "    </ul>\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EofHCraehRKL"
      },
      "source": [
        "Ejemplo para la tabla de resultados\n",
        "\n",
        "|Modelo|Procesado de imágenes|Diseño de red neuronal|Estrategia de entrenamiento|Acierto en test|\n",
        "|-------|---------------------|----------------------|---------------------------|---------------|\n",
        "|1|Tamaño 128, batch size 64|Convolutional(4= 1x 64; 3x 32) + Flatten + Dense(2064)|Entrenamiento desde 0|47,9%|\n",
        "|2|Tamaño 128, batch size 64|I.Augmentation + Convolutional(256) + Convolutional(126) + Convolutional(64) Flatten + Dense(2042)+ dropout(0,5)|Entrenamiento Bottleneck 0|28%|\n",
        "|pro|Tamaño 128, batch size 64|VGG16 + Global avergage pooling + Dense(4098) + Dense(2048)|Bottleneck features|79.5%|\n",
        "|pro2|Tamaño 128, batch size 64| Efficient_model + Global avergage pooling + Dense(2048) + Dense(1024) +  Dense(512) |Bottleneck features|82.13%|"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}